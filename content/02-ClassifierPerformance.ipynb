{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Performance: Classification\n",
    "\n",
    "Recall that after you train an ML model on past data, you can use that model to make predictions on new or previously unseen data. But how do you know if that model is useful?  In ML, when you hear the phrase \"model performance\", I want you to think about evaluating the quality of model predictions, commonly referred to as its **forecast skill** or **prediction skill**. This lesson focuses on evaluating prediction skill in the context of classification models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Types of Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Classification is a supervised machine learning problem. It is about predicting a label, typically a discrete value. For example, an image of an animal may be classified as being a picture of a _cat_ or _dog_. In machine learning, a problem like this that involves classifiying instances into one of two classes is called **binary classification**, while the term **multi-class classification** refers to classifying instances into one of three or more classes.\n",
    "\n",
    "<img style=\"align: center; margin: 15px 15px 15px 15px;\" src=\"../img/classification.png\" width=\"450\"/>\n",
    "\n",
    "Be careful not to confuse multi-class with **multi-label classification**, where multiple labels are predicted for each instance.  As shown in the figure below, you may have a binary classification setting where an email can be classified as either _spam_ or _not spam_. The picture in the middle shows a multi-class classification setting, where an animal can belong to one and only one class, in this case _dog_, even though there are multiple classes to choose from. On the right, you have a multi-label classification scenario where you may want to label the image with multiple classes based on a scene, for example, _horse_ and _dog_.\n",
    "\n",
    "<img style=\"align: center; margin: 15px 15px 15px 15px;\" src=\"../img/classificationtypes.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Classification Performance: Accuracy and Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "There are many ways to measure the prediction skill of a classification model, but **accuracy** and **error rate** are the de facto standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Accuracy\n",
    "Accuracy is the ratio of the correct predictions to the total number of predictions made.\n",
    "* Accuracy = Correct Predictions / Total Predictions\n",
    "\n",
    "90% and above for the accuracy of a predictive model is considered to be good, and it is common practice to aim for that level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Error Rate\n",
    "You can also summarize model performance in terms of the error rate.\n",
    "* Error Rate = Incorrect Predictions / Total Predictions\n",
    "\n",
    "Accuracy and error rates are complements of each other and therefore you can calculate one from the other as follows:\n",
    "* Accuracy = 1 - Error Rate\n",
    "* Error Rate = 1 - Accuracy\n",
    "\n",
    "Consider a classifier that labels pictures as either cats or dogs and that, when tested on 12 pictures (8 cats and 4 dogs), produces the following results:\n",
    "* 9 Correct Predictions   = (9/12) = 0.75 \n",
    "* 3 Incorrect Predictions = (3/12) = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "\n",
    "\n",
    "Knowing that the classifier has an accuracy of 0.75 or 75%, does not provide any insight into where the classifier is not performing well. Is it more mistaking cats for dogs, or dogs for cats? or is it about the same? This is where a **confusion matrix** may prove useful. \n",
    "\n",
    "A confusion matrix allows you to easily visualize classification performance.  \n",
    "\n",
    "<img src=\"../img/confusion_matrix.png\" width=\"200\">\n",
    "\n",
    "In this confusion matrix, of the 8 cat pictures, the model predicted that 2 were dogs, and of the 4 dog pictures, it predicted that 1 was a cat. All correct predictions are located in the diagonal of the table (highlighted in bold), so it is easy to visually inspect the table for prediction errors, as they are represented by values outside the diagonal. By examining the confusing matrix during development, you can see where the model may be confusing two or more classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Hands-On with Classification Performance: First Look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Scikit-learn is a free ML library for the Python programming language. It has 3 different programming interfaces for evaluating the quality of a modelâ€™s predictions:\n",
    "\n",
    "* Estimator Score Method\n",
    "* Scoring Parameter\n",
    "* Metrics Functions\n",
    "\n",
    "In this interactive demonstration, you'll get experience using the scikit-learn metrics functions to measure the prediction skill of a binary classifier that distinguishes cats and dogs. \n",
    "\n",
    "First start by importing the scikit-learn metrics module: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Assume that the actual and predicted values from the example are defined as follows, where cats belong to the class 0 and dogs belong to the class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actual_values = [0,0,0,0,0,0,0,0,1,1,1,1]\n",
    "predictions =   [1,1,0,0,0,0,0,0,1,1,1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now you can use the metrics functions to calculate the accuracy and print the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Accuracy: {metrics.accuracy_score(actual_values, predictions) * 100} % ')\n",
    "\n",
    "print(f'Confusion Matrix:')\n",
    "\n",
    "print(metrics.confusion_matrix(actual_values, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Classification Metrics: Precision, Recall, and F-Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "As a performance measure, classification accuracy has its limitations. One example where accuracy may be an inadequate performance measure is in the presence of class imbalance. For example, imagine a situation where a dataset of cat and dog images contains a large number of cat examples (majority class) and a small number of dog examples (minority class). On such a dataset, even unskillful models may achieve high accuracy if the large number of examples from the majority class overwhelms those in the minority class.  \n",
    "\n",
    "An alternative to using classification accuracy is to use precision and recall metrics. \n",
    "\n",
    "However, prior to getting into precision and recall, it is important to dive deeper into the confusion matrix as it provides insight into both the performance of the model and the types of errors being made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Confusion Matrix: Reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "The results summary displayed in the confusion matrix consists of true predictions and false predictions.\n",
    "\n",
    "<img src=\"../img/confusion_matrix_reloaded.png\" width=\"200\">\n",
    "\n",
    "True Predictions: \n",
    "  * TP: True Positives. \n",
    "    - Model predicted Yes, and actual value is Yes.\n",
    "  * TN: True Negatives. \n",
    "    - Model predicted No, and actual value is no.\n",
    "    \n",
    "False Predictions: \n",
    "  * FP: False Positives. \n",
    "    - Model predicted Yes, but actual value is No.\n",
    "  * FN: False Negatives. \n",
    "    - Model predicted No, but actual value is Yes.\n",
    "\n",
    "\n",
    "The **precision** and **recall** metrics are defined using the four terms (TP, TN, FP, and FN) in the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Precision quantifies the number of correct positive predictions made. It answers the question: When the model predicts yes, how often is it right? It is calculated as the ratio of correctly predicted positive examples divided by the total number of positive examples that were predicted.\n",
    "\n",
    "* Precision = True Positives / (True Positives + False Positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Recall quantifies the number of correct positive predictions made out of all of the positive predictions that could have been made. It answers the question: What percentage of the actual positives were identified? Therefore, unlike precision, recall provides an indication of missed positive predictions. It is calculated as the number of true positives divided by the total number of true positives and false negatives.\n",
    "\n",
    "* Recall = True Positives / (True Positives + False Negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "A predictive model with high recall and low precision returns many relevant results, but most of its predicted labels are incorrect. On the other hand, a predictive model with high precision and low recall returns very few relevant results, but most of its predicted labels are correct. The ideal predictive model has high precision and high recall, returning many relevant results with most of its results labeled correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### F-Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Precision and recall can be used to compute the **F-Measure** &mdash; a single metric that captures both properties. The traditional F measure is calculated as the harmonic mean of the two fractions.\n",
    "\n",
    "* F-Measure = (2 * Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "It is sometimes called the **F-Score** or **F1-Score** and is perhaps the most commonly used metric for imbalanced classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Hands-On with Classification Performance: Deep Dive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following code example demonstrates how precision, recall and the f1 score can be computed individually using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from sklearn import metrics\n",
    "\n",
    "#model prediction results\n",
    "actual_values = [0,0,0,0,0,0,0,0,1,1,1,1]\n",
    "predictions =   [1,1,0,0,0,0,0,0,1,1,1,0]\n",
    "\n",
    "#precision\n",
    "print(f'Precision Score is: {metrics.precision_score(actual_values, predictions)}')\n",
    "\n",
    "#recall\n",
    "print(f'Recall Score is: {metrics.recall_score(actual_values, predictions)}')\n",
    "\n",
    "#f1 score\n",
    "print('F1 Score:', metrics.f1_score(actual_values, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, instead of printing out individual metrics, you can view a full classification report using the scikit-learn `classification_report` function, which includes the accuracy, precision, recall and f-score all in one table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification report\n",
    "print(metrics.classification_report(actual_values, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise: Report on the Performance of a Spam Detection Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, you'll put everything you've learned so far about modeling actual and predicted values, and measuring classification performance in scikit-learn to the test.\n",
    "\n",
    "#### Problem Description:\n",
    "You are tasked with evaluating the performance of a predictive ML model for detecting email spam. \n",
    "\n",
    "The model is a binary classifier that distinguishes between email messages that are either **_spam_** or **_not spam_**.\n",
    "\n",
    "#### Data:\n",
    "\n",
    "The following table summarizes the performance data for this problem: \n",
    "<table align=\"left\" style=\"border-collapse:collapse;border-spacing:0\" class=\"tg\"><tbody><tr><td style=\"border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\"></td><td style=\"border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\"><span style=\"font-weight:bold\">Email 1</span></td><td style=\"border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\"><span style=\"font-weight:bold\">Email 2</span></td><td style=\"border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Email 3</td><td style=\"border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Email 4</td><td style=\"border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Email 5</td><td style=\"border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Email 6</td><td style=\"border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Email 7</td><td style=\"border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Email 8</td><td style=\"border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Email 9</td><td style=\"border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Email 10</td><td style=\"border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Email 11</td><td style=\"border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Email 12</td></tr><tr><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">Actual Values</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Spam</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Not Spam</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Not Spam</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Spam</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Spam</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Not Spam</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Spam</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Not Spam</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Not Spam</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Spam</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Not Spam</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Spam</td></tr><tr><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal\">Predictions</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Spam</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Spam</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Not Spam</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Spam</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Spam</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Not Spam</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Not Spam</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Not Spam</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Not Spam<br></td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Spam</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Spam</td><td style=\"border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal\">Spam</td></tr></tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions:\n",
    "Use the Python programming language, including the scikit-learn metrics library, to compute the following performance metrics, visualizations, and reports.\n",
    "\n",
    "* Accuracy, Error Rate, Confusion Matrix\n",
    "* Precision, Recall, F1 Score\n",
    "* Tabular Classification Performance Report\n",
    "\n",
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Probabilistic Forecasting in Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be situations where you want to predict the likelihood that a given observation belongs to each class in a classification problem rather than just trying predict a discrete response.  This is particularly useful for classification problems where there may be significant uncertainty. For example, some animals like leopards, jaguars, and cheetahs are often confused because they have similar appearances. In such cases, knowing the probability values that a given image is each type of \"big cat\" may be preferred over a direct classification.\n",
    "\n",
    "<img style=\"align: center; margin: 15px 15px 15px 15px;\" src=\"../img/similaranimals.jpeg\" width=\"320\"/>\n",
    "\n",
    "Another reason for this approach is to provide the capability to choose and even calibrate the threshold for how to interpret the predicted probabilities. For example, a default might be to use a threshold of 0.5, meaning that a probability in [0.0, 0.49] is a negative outcome (0) and a probability in [0.5, 1.0] is a positive outcome (1). Such a threshold can be adjusted to tune the behavior of the model for a specific problem. An example would be to reduce more of one or another type of error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Hands-On with Probabilistic Forecasting in Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Two diagnostic tools that help in the interpretation of probabilistic forecast for binary (two-class) classification predictive modeling problems are **precision-recall curves** and **receiver operating characteristic curves**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "In order to adequately evaluate a classifier, it is common to take both precision and recall into consideration. Unfortunately, improving precision typically reduces recall and vice-versa. A precision-recall curve is a diagram that shows the tradeoff between precision and recall. \n",
    "\n",
    "To plot a precision-recall curve you can use the scikit-learn function `plot_precision_recall_curve`. \n",
    "\n",
    "Up to this point you have been manually specifying model prediction results in an array. However, for this interactive demonstration you'll be going through the actual ML process, which involves:\n",
    "1. Preparing the training data\n",
    "2. Implementing the model and fitting it to the training data\n",
    "3. Using the model to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from math import sqrt\n",
    "from typing import List\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Prepare the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "actual_values = [1, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n",
    "predictions =   [1, 0, 1, 1, 1, 0, 1, 1, 0, 0]\n",
    "\n",
    "# Load Data\n",
    "data = pd.read_csv('../data/penguins_size.csv')\n",
    "\n",
    "data = data.dropna()  ## remove data entries with null values\n",
    "data = data.drop(['sex', 'island', 'flipper_length_mm', 'body_mass_g'], axis=1)  ## exclude specific rows from data\n",
    "data = data[data['species'] != 'Chinstrap']  ## select all species that are not Chinstrap\n",
    "\n",
    "X = data.drop(['species'], axis=1).values\n",
    "y = data['species']\n",
    "spicies = {'Adelie': -1, 'Gentoo': 1}\n",
    "y = [spicies[item] for item in y]\n",
    "y = np.array(y) \n",
    "\n",
    "# Split Data into Training Set and Test Set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Implement Model and Fit it to Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use a Decision Tree Classification Model\n",
    "dt_model = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Fit Model to Training Set\n",
    "dt_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Use Model to Make Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dt_predictions = dt_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Plot Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "disp = metrics.plot_precision_recall_curve(dt_model, X_test, y_test, color='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A high area under the curve represents both high precision and high recall, indicating that the classifier is not only returning a large percentage of relevant results, but it is also predicting the labels for them correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Receiver Operating Characteristic (ROC) Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When predicting the class label of a sample, an ML algorithm first calculates theÂ probabilityÂ that the processed sample belongs to a given class. If that value is above some predefinedÂ threshold, then it labels it as being a member of that class. For example, if for a given sample the algorithm predicts that there is a 0.7 (70%) chance that it isÂ Class 0,Â and threshold is 0.6, the sample will be labeled asÂ Class 0. This means that for different thresholds you can get different labels. \n",
    "\n",
    "The **receiver operating characteristic** (ROC) curve shows the true positive rate against the false positive rate for various thresholds.  You can plot the ROC curve using scikit-learn as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "metrics.plot_roc_curve(dt_model, X_test, y_test, color = 'orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC metric does not help you with model evaluation directly. In fact, the more interesting and useful aspect of the ROC curve as a measure of performance is the area under the curve (AUC). ROC is a probability curve whereas AUC represents the degree or measure of separability. The AUC-ROC combination tells you how good the model is at distinguishing between classes. The higher the AUC-ROC, the better the model is at predicting 0 classes as 0, and 1 classes as 1. You can calculate it using the scikit-learn function `roc_auc_score`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Area Under Curve for the Receiver Operating Characteristic\n",
    "print('AUC-ROC:', metrics.roc_auc_score(actual_values, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
