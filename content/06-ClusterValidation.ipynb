{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Clustering Validation\n",
    "\n",
    "When it comes to testing unsupervised machine learning, being able to validate the results of a clustering algorithm can be a very useful tool to have in your toolbelt. However, because of its very nature, cluster validation techniques are not as well-developed and practiced as those for classification. In this lesson, you'll learn why it is not easy to determine the quality of a clustering algorithm, and then jump into some of the common methods for cluster validation and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Types of Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Clustering involves discovering interesting patterns in data through grouping each data point into two or more buckets called **clusters**. Unlike classification that uses labeled data, clustering algorithms act on unlabeled data with the goal of assigning data points with similar properties and/or features to the same cluster. In other words, clustering techniques apply when there is no class to be predicted but rather when the instances are to be divided into natural groups.  There are many clustering algorithms to choose from and no single best clustering algorithm for all cases. Instead, it is a good idea to explore a range of clustering algorithms and different configurations for each algorithm. Here are 10 of the most popular clustering algorithms used today:\n",
    "\n",
    "* Affinity Propagation\n",
    "* Agglomerative Clustering\n",
    "* BIRCH\n",
    "* DBSCAN\n",
    "* K-Means\n",
    "* Mini-Batch K-Means\n",
    "* Mean Shift\n",
    "* OPTICS\n",
    "* Spectral Clustering\n",
    "* Mixture of Gaussians\n",
    "\n",
    "Although the validation techniques described in this training can be applied to different types of clustering algorithms, since you'll be applying them in the context of K-Means clustering, you may want to review the subsection below which provides specifics on the inner workings of that algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "K-Means is one of the most popular clustering algorithms. It uses an iterative technique to group unlabeled data into K clusters based on cluster centers referred to as **centroids**. The data in each cluster are chosen such that their average distance to their respective centroid is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Algorithm: How It Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The algorithm for k-means clustering is as follows:\n",
    "\n",
    "1. Randomly place K centroids for the initial clusters\n",
    "2. Assign each data point to their nearest centroid\n",
    "3. Update centroid locations based on the locations of the data points\n",
    "4. Repeat Steps 2 and 3 until points don’t move between clusters and centroids stabilize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Library Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Scikit-learn has a K-Means algorithm implementation that you can use instead of creating one from scratch. \n",
    "\n",
    "To use it: \n",
    "* Import the `KMeans()` method from the `sklearn.cluster` library to build a model with `n_clusters`\n",
    "* Fit the model to the data samples using `fit()` function\n",
    "* Predict the cluster that each data sample belongs to using `predict()` and store these as labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Validation Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Inertia** measures how well a dataset was clustered by K-Means. It is calculated by measuring the distance between each data point and its centroid, squaring this distance, and summing these squares across one cluster.\n",
    "\n",
    "A good model is one with low inertia AND a low number of clusters (K). However, this is a tradeoff because as K increases, inertia decreases.\n",
    "\n",
    "To find the optimal K for a dataset, you can use the **Elbow Method**. In the Elbow method, you are actually varying the number of clusters (K) from 1 – 10. For each value of K, you calculate the **Within-Cluster Sum of Square** (WCSS). The **WCSS** is the sum of squared distance between each point and the centroid in a cluster. When we plot the WCSS with the K value, the plot looks like an Elbow. As the number of clusters increases, the WCSS value will start to decrease. WCSS value is largest when K = 1. If you analyze the plot you will see that the graph will rapidly change at one of the points, creating an elbow shape. From this point, the graph starts to move almost parallel to the X-axis. It is the point where the decrease in inertia begins to slow, and the K value corresponding to it is the optimal number of clusters. For this graph, K=3 is the “elbow”.\n",
    "\n",
    "<img src=\"../img/elbowmethod.svg\" width=\"350\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Challenges and Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "From an intuitive point of view, the clustering problem has a clear goal: properly partitioning a set of unlabeled data. However, despite its intuitive appeal, the notion of [a cluster cannot be precisely defined](https://cs.nju.edu.cn/zhouzh/zhouzh.files/course/dm/reading/reading06/estivill-castro_sigkddexp02.pdf). As such, rather than thinking about a clustering algorithm or the clusters it produces as _right_ or _wrong_, you'll want to understand what makes a clustering solution _good_ or _bad_. You can even take this a step further by considering what characteristics a grouping problem should exhibit and can be considered \"good\", independently of the algorithm used to specify its solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Properties of Good Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Jon Kleinberg defined three axioms that describe the properties of \"good\" clustering.  These are:\n",
    "\n",
    "* **Scale Invariance**:  a clustering algorithm should not change its results when all distances between points are scaled by some constant factor.\n",
    "\n",
    "* **Consistency**: a clustering algorithm should not change its results if the distances within clusters decrease and/or the distances between clusters increase.\n",
    "\n",
    "* **Richness/Wealth**: a clustering algorithm must be flexible enough to produce any arbitrary partition of the input data set.\n",
    "\n",
    "Unfortunately, give the above properties Kleinberg shows that it is impossible for a clustering function to satisfy all three properties at once, referred to as the [Impossibility Theorem for Clustering](https://www.cs.cornell.edu/home/kleinber/nips15.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Analyzing Clustering Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the most important part of clustering validation is a proper analysis of the algorithm results.  This includes:\n",
    "\n",
    "1. Determining the best number of clusters.\n",
    "2. Assessing the quality of the results without using any external information.\n",
    "3. Comparing the results obtained with external information or oracles.\n",
    "4. Comparing two sets of clusters to determine which one is better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Internal Validation for Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internal validation methods make it possible to assess the quality of clustering without having access to external information, such as external labels that would be part of a supervised learning problem. In practice, two types of internal validation metrics are used: **cohesion** and **separation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Cohesion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measures how closely the elements of the same cluster are to each other.  It may be computed as the summation of the similarity between each pair of points within that cluster.\n",
    "\n",
    "<img  src=\"../img/cohesion.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measures how far apart clusters are from each other. Separation between two clusters is computed as the summation of the distance between each pair of points falling within the two clusters, given that both points are from different clusters.\n",
    "\n",
    "<img src=\"../img/separation.png\" width=\"570\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Combined Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of clusters having high cohesion within the clusters and high separation between the clusters is considered to be good. In practice, instead of dealing with two metrics, several measures are available which combine both of the above into a single measure. Few examples of such measures are:\n",
    "\n",
    "* Silhouette Coefficient\n",
    "* Calisnki-Harabasz Coefficient\n",
    "* Dunn Index\n",
    "* Xie-Beni Score\n",
    "* Hartigan Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Hands-On with Clustering Performance: Silhouette Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The silhouette coefficient _S<sub>i</sub>_ measures how similar an element _i_ is to the other elements in its own cluster versus those in a neighboring cluster. \n",
    "\n",
    "_S<sub>i</sub>_ values range from 1 to -1:\n",
    "* A value of _S<sub>i</sub>_ close to 1 indicates that the element _i_ is well clustered. In the other words, the element i is similar to the other elements in its group.\n",
    "\n",
    "* A value of _S<sub>i</sub>_ close to -1 indicates that the element _i_ is poorly clustered, and that assignment to some other cluster would probably improve the overall results.\n",
    "\n",
    "Plotting silhouette coefficients is a good way to validate parameters like the number of clusters visually. Sci-kit learn provides built-in support for silhouette analysis through the following `sklearn.metrics` library functions:\n",
    "\n",
    "* `silhouette_score`: returns the mean silhouette coefficient over all samples.  \n",
    "* `silhouette_samples`: returns the silhouette coefficient values for each sample.\n",
    "\n",
    "In this hands-on activity, you'll gain some practice using these functions to perform silhouette analysis as part of validating the number of clusters that should be used to produce \"good\" model performance.  You'll also learn how to create and interpret silhouette plots for the various clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Import and Configure Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the sample data from make_blobs\n",
    "# This particular setting has one distinct cluster and 3 clusters placed close together.\n",
    "X, y = make_blobs(n_samples=500,\n",
    "                  n_features=2,\n",
    "                  centers=4,\n",
    "                  cluster_std=1,\n",
    "                  center_box=(-10.0, 10.0),\n",
    "                  shuffle=True,\n",
    "                  random_state=1)  # For reproducibility\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Implement Model and Calculate Inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss=[]\n",
    "for i in range(1,11):\n",
    "    kmeans=KMeans(n_clusters=i, init='k-means++',random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Determine Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Plotting a visual of the explained variation against the number of clusters, you can use the elbow of the curve to determine the optimal number of clusters for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,11),wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Build Model with Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = KMeans(n_clusters=4, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Predict the Cluster for Each Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = clusterer.fit_predict(X)\n",
    "print(cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Perform Silhouette Analysis Across Clustering Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_clusters in range_n_clusters:\n",
    "    \n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # This first subplot is the silhouette plot\n",
    "    ax1.set_xlim([-0.1, 1])                              # In this example all the results lie within [-0.1, 1]\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])    # appends blank space between silhouette plots\n",
    "\n",
    "    # Initialize the clusterer, including a random seed for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # It provides insight into the cohesion and separation of the formed clusters\n",
    "    \n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # The second plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "  \n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the silhouette analysis is used to choose an optimal value for `n_clusters`. \n",
    "\n",
    "The silhouette plot shows that the `n_clusters` value of 3, 5 and 6 are a bad pick for the given data due to the presence of clusters with below average silhouette scores and also due to wide fluctuations in the size of the silhouette plots. \n",
    "\n",
    "Silhouette analysis is more ambivalent in deciding between 2 and 4.\n",
    "\n",
    "Also from the thickness of the silhouette plot the cluster size can be visualized. \n",
    "\n",
    "The silhouette plot for cluster 0 when `n_clusters` is equal to 2, is bigger in size owing to the grouping of the 3 sub clusters into one big cluster. However when the `n_clusters` is equal to 4, all the plots are more or less of similar thickness and hence are of similar sizes as can be also verified from the labeled scatter plot on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## External Validation for Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "External validation involves incorporating additional information, such as external class labels for the training examples, in the clustering validation process. Since unsupervised learning techniques are primarily used when such information is not available, external validation methods are not used on most clustering problems. However, they can still be applied if and when external information is available, or in cases where you generate synthetic data from a real data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Leveraging Subject Matter Experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Labels generated by subject matter experts (SMEs) can be used to represent the \"true\" labels for evaluating a clustering algorithm. \n",
    "\n",
    "Under this approach you have two sets of clusters: \n",
    "* S = { C<sub>1</sub>, C<sub>2</sub>, C<sub>3</sub>, …………, C<sub>n</sub> }, which has been generated as a result of clustering some data. \n",
    "* P = { D<sub>1</sub>, D<sub>2</sub>, D<sub>3</sub>, …………, D<sub>m</sub>}, which represent the true cluster labels provided by SMEs for grouping the same data. \n",
    "\n",
    "The approach to validation here is to measure the statistical similarity between the two sets. A cluster set C is considered as good if it is highly similar to the true class set P and, just as in classification problems, you can measure similarity by labeling results as:\n",
    "\n",
    "True Predictions:\n",
    "* TP. True Positive\n",
    "  - Number of data pairs found in the same group, both in C and in P.\n",
    "  \n",
    "* TN. True Negative\n",
    "  - Number of data pairs found in different groups, both in C and in P.\n",
    "  \n",
    "False Predictions:\n",
    "* FP. False Positive: \n",
    "  - Number of data pairs found in the same cluster in C, but different classes in P.\n",
    "  \n",
    "* FN. False Negative:\n",
    "  - Number of data pairs found in different clusters in C, but in the same classes in P."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### External Validation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "From these four indicators, you can then compute the performance measures to see how well the sets match, and the correlation between pairs. In addition, measures can also be applied from information theory. Here is a summary of each to point you in the directions for further study. Fortunately, you may already be familiar with a few from previous lessons and may of them also have built-in support using [scikit-learn metrics for clustering performance evaluation](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation).\n",
    "\n",
    "#### Matching Sets\n",
    "Measures for comparing the similarity between clusters in C and P are:\n",
    "* Precision: how many samples are properly classified within the same cluster.\n",
    "* Recall: what percentage of the relevant samples are included in the same cluster .\n",
    "* F-Score: combines precision and recall by calculating their weighted harmonic mean.\n",
    "\n",
    "#### Peer-to-Peer Correlation\n",
    "It is assumed that examples that are in the same cluster in C should be in the same class in P and vice-versa. As a result, external validation measures can be based on measuring the similarity between two partitions under equal conditions.  The following metrics are based on the correlation between pairs:\n",
    "* Jaccard Coefficient\n",
    "* Rand Coefficient\n",
    "* Fowlkes-Mallows Coefficient\n",
    "* Hubert Statistical Coefficient\n",
    "\n",
    "#### Information Theory\n",
    "Some external cluster validation meterics are based on information theory concepts such as the existing uncertainty in the prediction of the natural classes provided by partition P.  These include:\n",
    "* Entropy: a reciprocal measure of purity that allows you to measure the degree of disorder in the clustering results.\n",
    "* Mutual Information: measures the reduction in uncertainty about the clustering results given knowledge of the prior partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Cross-Over Validation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not to be confused with cross-validation, the term \"cross-over\" validation has been specifically coined in this training. As mentioned at the beginning of this section, clustering validation techniques are not as well developed and practiced as those for classification. This brings opportunities for the quality engineering and software testing communities to have approaches migrate or \"cross-over\" to the fields of ML and data engineering. To round up this lesson, you'll learn about an internal cluster validation approach that maps directly to a popular software testing technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Parallel System Validation in Software Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An engineering scenario that commonly occurs in the software industry is the replacement of a legacy system with a new application. With modern advances in software development, it is now possible to build a new application to replace a transactional legacy system in a fraction of the time it took to build and/or make changes to the original. However, constructing the new system is only half of the story. You still have to make sure that the new system works just as good, if not better, than the legacy system.\n",
    "\n",
    "One approach to checking that the behavior of the new system is at least as good as the existing system is to perform **parallel system validation**. The idea is that now there are two systems, written by different teams, using different code bases and technology stacks, but meant to conform to the same specification.  To validate the behavior of the new system, you use the existing legacy system as the baseline for quality. This involves running the both systems in parallel and comparing every line item, report, output, or result. If the output is the same, based on the same set of input test data, the new application is considered to be correct.\n",
    "\n",
    "<img style=\"align: center; margin: 15px 15px 15px 15px;\" src=\"../img/parallelvalidation.png\" width=\"600\"/>\n",
    "\n",
    "Testers can run years of historical transactions through the new application and reconcile the results in minutes thanks to improvements in computational power.  The end result is that most, if not all, of the use cases that the company faced in the last decade or so are tested down to the line-item level. If they match, there is a high degree of certainty that the parallel system runs just like the legacy one. For practical purposes, the parallel system is ‘proven’ to be a **twin** of the original.\n",
    "\n",
    "So you're probably wondering: what parallel system validation does this have to do with cluster validation?  A clustering validation technique called **twin sampling** makes it apparent that existing software testing approaches parallel system validation can be directly migrated to benefit AI and ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Twin Sample Validation for Clustering Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An approach to testing clustering algorithms in the absence of true cluster labels is twin sample validation. It assumes that you have already performed clustering on your training data and now want to validate the results. \n",
    "\n",
    "The approach consists of following four steps:\n",
    "\n",
    "1. Create a twin-sample of training data\n",
    "2. Perform unsupervised learning on twin-sample\n",
    "3. Import results for twin-sample from training set\n",
    "4. Calculate similarity between two sets of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create a Twin-Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The first step is to create a sample of data points which is expected to exhibit similar behavior to the training set. This is similar to the creation of a validation set in supervised learning, but with some additional constraints:\n",
    "\n",
    "* It should come from the same distribution as the training set.\n",
    "* It should sufficiently cover most of the patterns observed in the training set.\n",
    "* In case of the timeseries data:\n",
    "  - It should come from a different duration than the training set.   \n",
    "    Immediately succeeding is a good choice.\n",
    "  - It should cover at least one complete season of the data.  \n",
    "    For example, if the data has weekly seasonality, twin-sample should cover at least one complete week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Perform Clustering on Twin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a twin-sample, the next step is to perform clustering on it. For this, you should use the same parameters (number of clusters, distance metric, etc.) that you used on your training set. The output of this step will be a set of cluster labels, denoted as the output set S. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Import Results for Twin-Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute another set of cluster labels on the twin-sample using the training set. For each point in twin-sample, perform the following steps:\n",
    "\n",
    "1. Identify its nearest neighbor in the training set. \n",
    "2. Import the cluster label of its nearest neighbor.\n",
    "\n",
    "Following the above process, you should have a cluster label for each point in the twin sample, denoted as the set P."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Calculate Similarity of Twin and Original Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have two sets of cluster labels, S and P, for twin-sample, you can compute their similarity by using any external validation measure such as f-score, Jaccard Similarity, among others. A set of clusters having high similarity with its twin-sample is considered good. The idea here is that you should get similar results on our twin-sample set as you got on the training set, given that both these sets contain similar data and you are using the same parameter set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of software testing techniques that are good candidates to \"cross-over\" to the AI/ML world (not just clustering)?\n",
    "* Describe the testing technique and where it fits in the context of testing AI/ML.\n",
    "* How easy or how difficult would it be to tailor the approach to AI/ML?\n",
    "* Are there any special considerations or provisions that would have to be made before the cross-over could be realized?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
