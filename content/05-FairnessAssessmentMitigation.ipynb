{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AI Fairness\n",
    "\n",
    "Hold on to your data scientist hat but make room for your tester hat.  You'll need both for this section on AI/ML fairness. In these lessons and hands-on activities, you are going to learn how to assess ML models for fairness so that you can identify and quantify fairness-related risks, and employ strategies to mitigate them.  For the hands-on demos and exercises, you'll practice using data processing libraries like _pandas_ and _numpy_, along _seaborn_ for data visualization, to probe training data for fairness-related issues. You'll then get exposure to [Fairlearn](https://fairlearn.org/) and [AI Fairness 360](https://aif360.mybluemix.net/), open-source toolkits for measuring and mitigating fairness-related risks. Since testing is about risk and risk mitigation, the AI/ML and software testing worlds intersect quite naturally here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction to AI Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "AI is everywhere and in many ways this is great. However, at the same time, the opportunities brought about by AI also raise new challenges. Some of these challenges have received much attention in the media, and have highlighted the need to \"get AI right\".  Fairness in AI is about making sure that machines do not discriminate against certain groups of people, and possibly place already disadvantaged groups at a further disadvantage.  AI fairness is a fundamentally sociotechnical challenge, meaning that it cannot be approached from purely social or purely technical perspectives. Taken together, these factors can make it a pretty daunting landscape to navigate. Although there are few easy answers, there are a variety of strategies emerging for assessing and mitigating fairness-related risks, as well as a deepening understanding of the challenge throughout society. This tutorial explores these issues and gives you practice using open-source fairness toolkits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fairness Not Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Fairness is related, but distinct from anti-discrimination law. In this training, legal terminology like: _discriminate against_, _protected class_, _disparate treatment_ and _disparate impact_ are avoided. Although fairness is related to the concepts in anti-discrimination law, the goal is to focus on fairness and not touch on the legal considerations. It is important to note that some fairness interventions could be illegal, and, conversely, there are AI systems that follow the law, including antidiscrimination law, but still exhibit fairness issues.\n",
    "\n",
    "<img src=\"../img/fairnessnotlaw.png\" width=\"350\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Impacts and Risks Instead of Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "If you’ve read anything about fairness in AI systems then you’ve probably seen the word _bias_ get thrown around. In it's textbook definition, **bias** is a systemic or disproportionate tendency toward something or someone. When it comes to AI, bias is often used as a _catch-all_ phrase for describing any unfair system behavior, or causes of that unfairness. This training avoids using the word bias whenever possible because the term is ambiguous. Bias means very different things to different communities. For example, there is statistical bias, social bias, and systemic bias to name a few.  However, most issues typically arise at the intersection of different types of bias.  \n",
    "\n",
    "<img src=\"../img/biasintersection.png\" width=\"300\"/>\n",
    "\n",
    "For this reason, it is better to talk about the **impacts** or **risks** related to fairness in AI-based systems. Such terms are not only useful for referring to those who may be harmed by the system and in what ways, but also for describing assessment and mitigation strategies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Debiasing? No Such Thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "I also want to emphasize that because there are so many different reasons why AI systems can cause fairness-related issues, it is not possible to completely remove bias from a system or guarantee its fairness. As a result, throughout this training you aren't likely to encounter terms like **debiasing** or **unbiased**, as they tend to set unrealistic expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluating Fairness in AI and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The goal of evaluating fairness in AI/ML is to answer the questions:\n",
    "* Are there groups of people who are disproportionately, negatively impacted by the system?\n",
    "* If so, in what ways are they impacted and what can we do about it?\n",
    "\n",
    "The steps in AI fairness validation are identifying risks or potential harm, determining which groups might be impacted, quantifying risks and comparing them across groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Identifying Risks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "There are three broad categories of fairness-related risks that can occur in ML models: **allocation risk**, **quality of service risk**, and **representation risk**.  Note that these are not mutually exclusive categories and therefore it is possible for a single AI system to exhibit more than one type of potentially harmful behavior.\n",
    "\n",
    "#### Allocation Risk\n",
    "\n",
    "This type of risk may occur when AI systems are used allocate opportunities or resources in ways that\n",
    "can have significant negative impacts on people’s lives. For example, Amazon abandoned its automated hiring system after finding that it amplified the existing gender imbalance in the tech industry by withholding employment opportunities\n",
    "from women.\n",
    "\n",
    "#### Quality of Service Risk\n",
    "Quality of service risk is about whether a system works as well for one person as it does for another. For example, researchers found that 3 commercial gender classifiers had higher error rates for images of women with darker skin tones, than for images of men with lighter skin tones. Like accessibility issues, quality of service risks can raise questions about respect, dignity, and personhood. Imagine how a user might feel if a system repeatedly fails to recognize her voice, but easily recognizes those of her peers?\n",
    "\n",
    "#### Representation Risk\n",
    "Representation risks include things like stereotyping, denigration, and any form of over- or under-representation. Denigration occurs when an AI system is itself part of a process that is actively derogatory, demeaning, or offensive. For example, Google Photos infamously mislabeled an image of Black people as _gorillas_. A similar scenario happened again a few years later when Facebook's AI labeled black men in a video as _primates_. This mislabeling is harmful not just because these system made the same mistake, but because the labels they applied have a history of being purposefully used to denigrate and demean Black people.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Determining Impacted Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "With potential fairness-related harms identified, it's time to determine who may be at risk of experiencing them. Many times it is the people who will use or operate the system, but somtimes it can be other stakeholders that are directly or indirectly impacted, either by choice or not. For example, in the case of a facial recognition system for workplace building access, the system operator is not the person whose face is being recognized and thus not the person who is most immediately harmed if the system makes a mistake.\n",
    "\n",
    "#### Search for the Most Relevant Groups\n",
    "Although news and media stories often focus on groups of people that are protected by antidiscrimination laws, such as groups defined in terms of race, gender, age, or disability status, there are actually many different groups of people that we want our systems to treat fairly and it’s not always easy to identify the most relevant ones, which can even be specific to the domain or use case. For example, in the case of an automated essay-grading system, whether someone is a native speaker of the language may be more relevant than their age or their disability status.\n",
    "\n",
    "#### Carefully Examine Group Intersections\n",
    "Remember that groups intersect and people at those intersections may be at risk of experiencing unique harms that might be obscured by considering only non-intersected groups. Returning to the image-based gender classifiers mentioned earlier, error rates were significantly higher for women with darker skin tones than for images of women overall, or for images of people with darker skin tones overall. The only way to discover those types of issues are consider the models' performance with respect to skin tone and gender at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Quantifying Risks and Comparing Across Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Risk quantification is a well-studied subject and you can probably find many tools and resources to help you with this step. The two key components of risk are the **likelihood** (probability) that a given event occurs, and the **severity** of any negative impacts. A common practice for quantifying risks is to use a **risk assessment matrix**.  As shown in the matrix, if there is a high probablity that a risk, if realized, will have a sever impact, then that item should have the attention of the team.  In software testing, the idea behind **risk-based testing** is that you should spend more time validating those aspects of the system that pose the highest threat, and such a philosophy is definitely applicable to testing AI for fairness.\n",
    "\n",
    "<img src=\"../img/riskmatrix.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Two critical decisions when desiging an AI system are\n",
    "\n",
    "how we define the machine learning task\n",
    "what dataset we use to train our models\n",
    "These choices are often intertwined, because the dataset is often a convenience dataset, based on availability, which leads to a specific choice of label and performance metric (that's also the case in our scenario).\n",
    "\n",
    "In this part of the tutorial, we first load the dataset, and then we examine it for a variety of fairness issues:\n",
    "\n",
    "sample sizes of different demographic groups, and in particular different racial groups\n",
    "appropriateness of our choice of label (readmission within 30 days)\n",
    "representativeness/informativeness of different features for different groups\n",
    "Besides dataset characteristics, one additional aspect of dataset fairness is whether the data was collected in a manner that respects the autonomy of individuals in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Context for Hands-On Activities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "The hands-on interactive demonstrations, practices, and exercises in this notebook are all based on the information in this section. Although each hands-on practice builds on the previous, any required code sections have been replicated within each section so that they are all self-contained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Consider an automated system for recommending patients for _high-risk care management_ programs.\n",
    "\n",
    "> These programs seek to improve the care of patients with complex health needs by providing additional resources, including greater attention from trained providers, to help ensure that care is well coordinated. Because the programs are themselves expensive with costs going toward teams of dedicated nurses, extra primary care appointment slots, and other scarce resources — **the automated system will rely extensively on ML algorithms to identify patients who will benefit the most**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "The proposed automated system in the scenario will be implemented as an ML classification model.\n",
    "\n",
    "> The purpose of the classifier is to predicts whether a patient should be suggested to their primary care physicians for enrollment into the care management program. A positive prediction will mean recommendation into the care program.  Since hospital **readmission within 30 days** can be viewed as a proxy that the patients needed more assistance at the release time, it has been selected as the label (output) the model will be trained to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The dataset for this scenario is a [publicly available clinical dataset](https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008).\n",
    "\n",
    ">The data focuses on hospital re-admissions for _diabetic patients_ across 130 different hospitals in the U.S., recorded over a ten-year period. Each record represents a patient's readmission whose stay lasted one to fourteen days. The features describing each re-admission encounter include **demographics**, **diagnoses**, **diabetic medications**, **number of visits** in the preceding year, **payer information**, and whether the patient was **readmitted after release**, and if **readmission was within 30 days** of the release."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hands-On with Testing Fairness in Training Data: Sample Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Import and Configure Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing: numpy, pandas\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.float_format\", \"{:.3f}\".format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/diabetic_preprocessed.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task Description and Interactive Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Examine the dataset with an eye towards a common threat to fairness — the sample sizes for different demographic groups. \n",
    "\n",
    "#### Sample Sizes for Gender (Counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"gender\"].value_counts().plot(kind='bar', rot=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Sample Sizes for Gender (Frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"gender\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fairness Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Small sample sizes have two implications:\n",
    "* **Training**: Fewer training samples may result in the model failing to learn patterns related to smaller groups, which means that its predictive performance on these groups could be worse.\n",
    "* **Assessment**: Fewer data points overall may mean a much larger uncertainty (error bars) in model estimates, making the impacts of the system on smaller groups harder to assess. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Write code to **print** and **plot** the sample sizes of the **race** demographic group using either counts or frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Your Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sample Sizes Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are your thoughts on the distribution for race?  \n",
    "\n",
    "Modify your code above so that you can examine sample sizes for age instead of race.   \n",
    "What risks if any are associated with age?  \n",
    "\n",
    "Using a scale of _high_, _medium_, _low_, rate the risks associated with each feature as pertains to sample sizes? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Mitigating the Risks of Small Sample Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "When the data set lacks coverage of certain groups, it means that you won't be able to reliably assess any fairness-related issues. There are three possible interventions, which can be combined if needed. These are:\n",
    "\n",
    "* **Collecting**: gather more data for groups with fewer samples.\n",
    "* **Pooling**: merge some of the groups with fewer samples together to make a larger group.\n",
    "* **Dropping**: removing some of the group with fewer samples altogether (extra caution).\n",
    "\n",
    "The choice of strategy depends on your existing understanding of which groups are at the greatest risk of harm. Pooling the groups with widely different risks could mask the extent of harms. You should be extra  cautious against dropping small groups completely as this may leads to the representational harm of erasure. If any groups are merged or removed, these decisions should be annotated and explained. Experimentation is also a useful tool in your toolbelt, so I recommend to approach these issues as both a scientist and a software engineer/tester. For the previous exercise, the recommendations are:\n",
    "\n",
    "* Drop the gender group labeled *Unknown/Invalid* because the sample size is so small that no meaningful fairness assessment is possible.\n",
    "\n",
    "* Merge the three smallest race groups *Asian*, *Hispanic*, *Other*, but also retain the original groups for auxiliary assessments. \n",
    "\n",
    "Here is the code that implements these recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop gender group Unknown/Invalid\n",
    "df = df.query(\"gender != 'Unknown/Invalid'\")\n",
    "\n",
    "# retain the original race as race_all, and merge Asian+Hispanic+Other \n",
    "df[\"race_all\"] = df[\"race\"]\n",
    "df[\"race\"] = df[\"race\"].replace({\"Asian\": \"Other\", \"Hispanic\": \"Other\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hands-On with Testing Fairness in Training Data: Label Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Before you can adequately evaluate the care program recommendation model for fairness, you must check whether the choice of classification label, **readmission within 30 days**, aligns with the expectations that the system shall **identify patients that would benefit** from the care management program. In other words, the following assumption was made during model development: \n",
    "\n",
    ">The **greatest benefit** from the care management program **would go to patients most likely to be readmitted** within 30 days. \n",
    "\n",
    "Now you need to test that assumption!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task Description: Predictive Validity Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Formally stated, the task is to investigate whether the selected measurement `readmit_30_days` correlates with patient characteristics that support the construct **benefit from care management**. One such characteristic is general patient health, where it is reasonable to expect that patients that are less healthy, are more likely to benefit from care management. Although the dataset does not contain full health records for measuring general patient health, it does contain two relevant features on a patient's health during the preceding year.  These are:\n",
    "\n",
    "* `had_emergency`: indicates whether the patient spent any days in the hospital's emergency room. \n",
    "* `had_inpatient_days`: indicates whether the patient spent any days in the hospital that were non-emergency.\n",
    "\n",
    "To test the validity of the classification (output) label, you'll want to check that `readmit_30_days` is predictive of the two observable characteristics `had_emergency` and `had_inpatient_days`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Import and Configure Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Processing: numpy, pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.float_format\", \"{:.3f}\".format)\n",
    "\n",
    "# Plotting: seaborn\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/diabetic_preprocessed.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  Interactive Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Start by examining the rate at which patients with different `readmit_30_days` labels were readmitted in the previous year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot showing how predictive readmit_30_days is of had_emergency\n",
    "sns.pointplot(y=\"had_emergency\", x=\"readmit_30_days\",\n",
    "              data=df, ci=95, join=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "The plot shows that patients with `readmit_30_days=0` have a lower rate of emergency visits in the prior year, whereas patients with `readmit_30_days=1` have a larger rate. The vertical lines indicate 95% confidence intervals obtained via a method called [bootstrapping](https://machinelearningmastery.com/a-gentle-introduction-to-the-bootstrap-method/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Now look at how `readmit_30_days` performs when predicting the rate of (non-emergency) hospital visits in the previous year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot showing how predictive readmit_30_days is of had_emergency\n",
    "sns.pointplot(y=\"had_emergency\", x=\"readmit_30_days\",\n",
    "              data=df, ci=95, join=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Do you see a similar pattern when `readmit_30_days` is used to predict the rate of non-emergency hospital visits when compared to emergency visits? If so, great! However, the test isn't over yet as you still have to consider fairness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fairness Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Although the predictiveness of the model may be valid overall, it does necessarily mean that it is similar across different groups. To gain insights on the predictiveness across groups you can use create a categorical pointplot using Seaborn. Here is an example that visualizes the `readmit_30` label's predictiveness of `had_emergency` and `had_inpatient_days` across **race**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictiveness using a categorical pointplot\n",
    "sns.catplot(y=\"had_emergency\", x=\"readmit_30_days\", hue=\"race\", data=df,\n",
    "            kind=\"point\", ci=95, dodge=True, join=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The patients in the group Unknown have a substantially lower rate of emergency visits in the prior year, regardless of whether they are readmitted in 30 days. The readmission is still positively correlated with `had_emergency`, but note the large error bars (due to small sample sizes).\n",
    "\n",
    "We also see that the group with feature value AfricanAmerican has a higher rate of emergency visits compared with other groups. However, generally the groups Caucasian, AfricanAmerican and Other follow similar dependence patterns.\n",
    "\n",
    "We see a similar pattern when readmit_30_days is used to predict the rate of (non-emergency) hospital visits in the previous year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y=\"had_inpatient_days\", x=\"readmit_30_days\", hue=\"race\", data=df,\n",
    "            kind=\"point\", ci=95, dodge=True, join=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, for Unknown the rate of (non-emergency) hospital visits in the previous year is lower than for other groups.In all groups there is a strong positive correlation between `readmit_30_days` and `had_inpatient_days`.\n",
    "\n",
    "In all cases, we see that `readmission in 30 days` is predictive of our two measurements of general patient health.\n",
    "\n",
    "The analysis is also surfacing the fact that patients with the value of race Unknown have fewer hospital visits in the preceding year (both emergency and non-emergency) than other groups. In practice, this would be a good reason to reach out to health professionals to investigate this patient cohort, to make sure that we understand why there is the systematic difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the model's predictive validity with respect to `gender` and `age`. Create visuals to help with your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Your Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Discussion\n",
    "\n",
    "Do you see any differences between the two sets of visualization results?  \n",
    "\n",
    "If there's a difference, can you form a hypothesis about why it may be occuring?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hands-On with Fairness Toolkits: Measuring Fairness Risks with Fairlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Google, Microsoft and IBM are all investing in AI fairness toolkits. The tools in these toolkits generally provide support for:\n",
    "* Visually probing datasets and the output of your ML models \n",
    "* Detecting and mitigating fairness-related risks\n",
    "* Understanding any trade-offs between fairness and performance.\n",
    "\n",
    "### [Fairlearn](https://fairlearn.org)\n",
    "\n",
    "It's time for hands-on practice using one such toolkit &mdash; **Fairlearn**.  The Fairlearn project originally started in 2018 at Microsoft Research, but since 2021 has adopted a neutral governance structure and is now completely community-driven. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <a id='fairness_toolkit_setup'>Setup</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### <a id='fairness_toolkit_setup_import'>Import and Configure Libraries</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing: Numpy, Pandas, Scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.float_format\", \"{:.3f}\".format)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import Bunch\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    plot_roc_curve)\n",
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting: Seaborn, Matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Fairness: Fairlearn\n",
    "\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame,\n",
    "    true_positive_rate,\n",
    "    false_positive_rate,\n",
    "    false_negative_rate,\n",
    "    selection_rate,\n",
    "    count,\n",
    "    false_negative_rate_difference\n",
    ")\n",
    "\n",
    "from fairlearn.postprocessing import ThresholdOptimizer, plot_threshold_optimizer\n",
    "from fairlearn.postprocessing._interpolated_thresholder import InterpolatedThresholder\n",
    "from fairlearn.postprocessing._threshold_operation import ThresholdOperation\n",
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds, TruePositiveRateParity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### <a id='fairness_toolkit_setup_data'>Prepare Training and Test Data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the Dataset\n",
    "df = pd.read_csv(\"../data/diabetic_preprocessed.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mark all the category features\n",
    "categorical_features = [\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"age\",\n",
    "    \"discharge_disposition_id\",\n",
    "    \"admission_source_id\",\n",
    "    \"medical_specialty\",\n",
    "    \"primary_diagnosis\",\n",
    "    \"max_glu_serum\",\n",
    "    \"A1Cresult\",\n",
    "    \"insulin\",\n",
    "    \"change\",\n",
    "    \"diabetesMed\",\n",
    "    \"readmitted\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col_name in categorical_features:\n",
    "  df[col_name] = df[col_name].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop gender group Unknown/Invalid\n",
    "df = df.query(\"gender != 'Unknown/Invalid'\")\n",
    "\n",
    "# retain the original race as race_all, and merge Asian+Hispanic+Other \n",
    "df[\"race_all\"] = df[\"race\"]\n",
    "df[\"race\"] = df[\"race\"].replace({\"Asian\": \"Other\", \"Hispanic\": \"Other\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Target variable is readmission within 30 days, and sensitive feature for fairness assessment is race.\n",
    "target_variable = \"readmit_30_days\"\n",
    "demographic = [\"race\", \"gender\"]\n",
    "sensitive = [\"race\"]\n",
    "\n",
    "# If multiple sensitive features are chosen, the rest of the script considers intersectional groups.\n",
    "Y, A = df.loc[:, target_variable], df.loc[:, sensitive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the features that won't be used and expand the categorical features into 0/1 indicators (\"dummies\")\n",
    "X = pd.get_dummies(df.drop(columns=[\n",
    "    \"race\",\n",
    "    \"race_all\",\n",
    "    \"discharge_disposition_id\",\n",
    "    \"readmitted\",\n",
    "    \"readmit_binary\",\n",
    "    \"readmit_30_days\"\n",
    "]))\n",
    "\n",
    "X.head() # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 445\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test, A_train, A_test, df_train, df_test = train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    A,\n",
    "    df,\n",
    "    test_size=0.50,\n",
    "    stratify=Y,\n",
    "    random_state=random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_dataset(X_train, Y_train, A_train):\n",
    "\n",
    "  negative_ids = Y_train[Y_train == 0].index\n",
    "  positive_ids = Y_train[Y_train == 1].index\n",
    "  balanced_ids = positive_ids.union(np.random.choice(a=negative_ids, size=len(positive_ids)))\n",
    "\n",
    "  X_train = X_train.loc[balanced_ids, :]\n",
    "  Y_train = Y_train.loc[balanced_ids]\n",
    "  A_train = A_train.loc[balanced_ids, :]\n",
    "  return X_train, Y_train, A_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bal, Y_train_bal, A_train_bal = resample_dataset(X_train, Y_train, A_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train the Model using the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmitigated_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessing\", StandardScaler()),\n",
    "    (\"logistic_regression\", LogisticRegression(max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmitigated_pipeline.fit(X_train_bal, Y_train_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_proba = unmitigated_pipeline.predict_proba(X_test)[:,1]\n",
    "Y_pred = unmitigated_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Check Model Performance on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve of probabilistic predictions\n",
    "plot_roc_curve(unmitigated_pipeline, X_test, Y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show balanced accuracy rate of the 0/1 predictions\n",
    "balanced_accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct a **fairness assessment on the ML-based care program recommendation system** described in the scenario. To keep the task manageable, the scope of the assessment can be limited to identifying and quantifying fairness-related risks for one group that might be harmed by the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the steps in assessing fairness are: identify risks (potential harm), determine which groups might be impacted, quantify risks and compare quantified risks across groups. In this section, I'll walk you through each of these steps and make concrete choices on various aspects of the assessment, so that you can see what is necessary to complete the task.  Immediately afterwards, you'll get hands-on practice using Fairlearn to support these activities.\n",
    "\n",
    "#### Risks\n",
    "For the healthcare scenario, the patients that would benefit from the care management program, but are not recommended for it, experience **allocation harm**. In the context of the trained classifier, these are any **false negative** results. \n",
    "\n",
    "#### Potentially Impacted Groups\n",
    "When assessing fairness you should consider demographics, including historically marginalized groups like those based on gender, race and ethnicity. It is also important to consider group intersections, e.g., Black Latin Women. This demonstration will focus on groups defmined by **race**.\n",
    "\n",
    "#### Quantified Risks\n",
    "Two metrics will be used for quantifying fairness-related risks:\n",
    "\n",
    "* **False Negative Rate**: Fraction of patients that are readmitted within 30 days, but that are not recommended for the care management program; this quantifies harm.\n",
    "\n",
    "* **Selection Rate**: Fraction of patients that are recommended for the care management program regardless of whether they are readmittted with 30 days or not. Technically, as stated, this quantifies a benefit. However, the assumption is that all patients should benefit similarly from the extra care, but if that does not happen it represents a fairness-related risk.\n",
    "\n",
    "#### Risk Comparison Across Groups\n",
    "The workhorse of fairness assessment are _disaggregated metrics_, which are **metrics evaluated on slices of data**. For example, to measure harms due to errors, you would begin by evaluating the errors on each slice of the data that corresponds to potentially impacted groups. If some of the groups are seeing much larger errors than other groups, flag this as a fairness harm.\n",
    "\n",
    "To summarize the disparities in errors, or other metrics, you may want to report quantities such as the **difference** or **ratio** of the metric values between the best and the worst slices. In settings where the goal is to guarantee certain minimum quality of service, it is also meaningful to report the **worst performance** across all considered groups. For example, when comparing false negative rate across groups defined by race, the findings can be summarized in a table like this one:\n",
    "\n",
    "| | false negative rate<br>(FNR) |\n",
    "|---|---|\n",
    "| AfricanAmerican | 0.43 |\n",
    "| Caucasian | 0.44 |\n",
    "| Other | 0.52 |\n",
    "| Unknown | 0.67 |\n",
    "| | |\n",
    "|_largest difference_| 0.24 &nbsp;&nbsp;(best is 0.0)|\n",
    "|_smallest ratio_| 0.64 &nbsp;&nbsp;(best is 1.0)|\n",
    "|_maximum_<br>_(worst-case) FNR_|0.67|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Interactive Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fairlearn provides the data structure called `MetricFrame` to enable evaluation of disaggregated metrics. In this interactive hands-on demo, you'll learn how to use the `MetricFrame` object to assess fairness-related risks in a trained `LogisticRegression` classifier for the scenario.\n",
    "\n",
    "**Reminder**: Make sure you have run all the steps in the [Setup section](#fairness_toolkit_setup) before proceeding with this demo. \n",
    "\n",
    "#### Using MetricFrame\n",
    "In its simplest form MetricFrame takes four arguments:  \n",
    "\n",
    "* A metrics function: `metric_function(y_true, y_pred)`\n",
    "* An array of labels: `y_true`\n",
    "* An array of predictions: `y_pred`\n",
    "* An array of sensitive features: `sensitive_features`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf1 = MetricFrame(metrics=false_negative_rate,\n",
    "                  y_true=Y_test,\n",
    "                  y_pred=Y_pred,\n",
    "                  sensitive_features=df_test['race'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Accessing the Metrics in MetricFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any **disaggregated metrics** are stored in a Pandas Series `mf1.by_group`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf1.by_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **largest difference**, **smallest ratio** and **worst-case performance** are accessed as:\n",
    "\n",
    "* `mf1.difference()`\n",
    "* `mf1.ratio()`\n",
    "* `mf1.group_max()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"difference: {mf1.difference():.3}\\n\"\n",
    "      f\"ratio: {mf1.ratio():.3}\\n\"\n",
    "      f\"max across groups: {mf1.group_max():.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also **evaluate multiple metrics** by providing a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {\n",
    "    \"selection_rate\": selection_rate,\n",
    "    \"false_negative_rate\": false_negative_rate,\n",
    "    \"balanced_accuracy\": balanced_accuracy_score,\n",
    "}\n",
    "\n",
    "metricframe_unmitigated = MetricFrame(metrics=metrics_dict,\n",
    "                  y_true=Y_test,\n",
    "                  y_pred=Y_pred,\n",
    "                  sensitive_features=df_test['race'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After which, the disaggregated metrics are stored in a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricframe_unmitigated.by_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the largest difference, smallest ratio, and the maximum and minimum **values across the groups** are then all pandas Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricframe_unmitigated.difference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view them transposed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'difference': metricframe_unmitigated.difference(),\n",
    "              'ratio': metricframe_unmitigated.ratio(),\n",
    "              'group_min': metricframe_unmitigated.group_min(),\n",
    "              'group_max': metricframe_unmitigated.group_max()}).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also easily plot all of the metrics using DataFrame plotting capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricframe_unmitigated.by_group.plot.bar(subplots=True, layout= [1,3], figsize=(12, 4),\n",
    "                      legend=False, rot=-45, position=1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the fun part. Analyzing the fairness measurements produced by Fairlearn or your toolkit of choice. Here's the analysis for the healthcare scenario:\n",
    ">According to the above bar chart, it appears that the group *Unknown* is selected for the care management program less often than other groups as reflected by the selection rate. Also this group experiences the largest false negative rate, so a larger fraction of group members that are likely to benefit from the care management program are not selected. Finally, the balanced accuracy on this group is also the lowest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, there seems to be disparity. But why? There's a variety of reasons why such disparities may occur:\n",
    "* **Representational issues**: not enough instances per group\n",
    "* **Feature distribution differs across groups**: differences in the relationship between features and target variable\n",
    "\n",
    "Real-world applications often exhibit both kinds of issues at the same time. The key takeaway is that by measuring fairness using tools like Fairlearn you can identify such disparities and determine whether or not they pose a serious risk to the project.  If they do, you'll want to employ some sort of mitigation strategy reduce the likelihood and/or severity of fairness-related harm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you'll be training your own model and evaluating it for fairness-related risks using `MetricFrame`.  I encourage you to explore the model's performance across different sensitive features, such as `age` or `gender`, as well as different model performance metrics. A gentle reminder that if you jumped straight to this exercise, you'll want to run the [import and configure libraries](#fairness_toolkit_setup_import) and [prepare training and test data](#fairness_toolkit_setup_data) sections before you begin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Step 1** - Train a `HistGradientBoostingClassifier`, and fit it to the balanced training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# Create your model here\n",
    "clf = HistGradientBoostingClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(__________, ________)\n",
    "exercise_pred = clf.predict(______)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2** - Evaluate the fairness of the model using `MetricFrame`. \n",
    "\n",
    "First, define a `MetricFrame` that looks at the following metrics:\n",
    "*   **Count**: The number of data points belonging to each sensitive feature category.\n",
    "*   **False Positive Rate**:  False Negatives / (False Negatives + False Positives)\n",
    "\n",
    "*  **Recall Score**: True Positives / (True Positives + False Negatives)\n",
    "\n",
    "As an extra challenge, you can use the prediction probabilities to compute the **ROC AUC Score** for each sensitive group pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define exercise fairness metrics of interest here\n",
    "exercise_metrics = {\n",
    "    \"count\": count,\n",
    "    \"false_positive_rate\": _______,\n",
    "    \"recall_score\": _______\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create the `MetricFrame` using the metrics listed above with the sensitive groups of `race` and `gender`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricframe_exercise = MetricFrame(\n",
    "    metrics=__________,\n",
    "    y_true=Y_test,\n",
    "    y_pred=__________,\n",
    "    sensitive_features=_____\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Step 3** - Explore and play around with the plotting capabilities of the `MetricFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricframe_exercise._________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some of the performance disparities here\n",
    "metricframe_exercise.by_group.____.bar(subplots=_____, layout=[1,3], figsize=(12, 4),\n",
    "                                       legend=False, rot=-45, position=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the charts above are based on test data, so without any uncertainty quantification like error bars or confidence intervals, you cannot reliably compare these statistics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hands-On with Fairness Toolkits: Mitigating Fairness Risks with AI Fairness 360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### [AI Fairness 360](https://aif360.mybluemix.net/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "AI Fairness 360 (AIF360) is an extensible open source toolkit that can help you examine, report, and mitigate fairness-related risks, referred to as **bias** on their site, in ML models. AI Fairness 360 was created by IBM Research and donated by IBM to the Linux Foundation AI & Data.\n",
    "\n",
    "AIF360 includes a [Python package](https://pypi.org/project/aif360/) that has a comprehensive set of metrics for datasets and models to test for biases, explanations for these metrics, and algorithms to mitigate bias in datasets and models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task Description and Interactive Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Since you have already explored the Python library for Fairlearn, in this hands-on task you'll interact with the AI Fairness 360 toolkit via a web-based demo.\n",
    "\n",
    "<img style=\"align: center; margin: 15px 15px 15px 15px; border:1px solid black;\"  src=\"../img/aif360.gif\" width=\"800\"/>\n",
    "\n",
    "##### **Step 1.** Navigate to the [AIF360 Interactive Demo](https://aif360.mybluemix.net/data) website.\n",
    "\n",
    "##### **Step 2.** Walk through the process and apply mitigation strategies to different datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "What are your thoughts on some of the different mitigation strategies and the visualizations?\n",
    "* **Reweighing**: modifies the weights of different training examples\n",
    "* **Optimized Pre-Processing**: modifies training data labels and features\n",
    "* **Adversarial Debiasing**: leverages models that compete with each other\n",
    "* **Reject Option-Based Classification**: changes predictions from a classifier to make them fairer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
